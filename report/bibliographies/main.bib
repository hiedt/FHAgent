@misc{weber2022ss,
      title={Steady-State Error Compensation in Reference Tracking and Disturbance Rejection Problems for Reinforcement Learning-Based Control}, 
      author={Daniel Weber and Maximilian Schenke and Oliver Wallscheid},
      year={2022},
      eprint={2201.13331},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}
@misc{hare2019dealing,
      title={Dealing with Sparse Rewards in Reinforcement Learning}, 
      author={Joshua Hare},
      year={2019},
      eprint={1910.09281},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{article,
    author = {Wetter, Michael},
    year = {2009},
    month = {01},
    pages = {},
    title = {A Modelica-based Model Library for Building Energy and Control Systems}
}
@misc{bansal2017mbmf,
      title={MBMF: Model-Based Priors for Model-Free Reinforcement Learning}, 
      author={Somil Bansal and Roberto Calandra and Kurtland Chua and Sergey Levine and Claire Tomlin},
      year={2017},
      eprint={1709.03153},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{ISSN0360-5442,
    title = "Data-Driven Offline Reinforcement Learning for HVAC-Systems",
    author = "Christian Blad and Simon B{\o}gh and Carsten Kalles{\o}e",
    year = "2022",
    month = {12},
    day = "15",
    doi = "10.1016/j.energy.2022.125290",
    language = "English",
    volume = "261",
    journal = "Energy",
    issn = "0360-5442",
    publisher = "Pergamon Press",
    number = "Part B",
}
@inproceedings{chen2019gnu,
    title={Gnu-RL: A Precocial Reinforcement Learning Solution for Building HVAC Control Using a Differentiable MPC Policy},
    author={Chen, Bingqing and Cai, Zicheng and Berg{\'e}s, Mario},
    booktitle={Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
    pages={316--325},
    year={2019},
    organization={ACM}
}
@misc{td3,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{kim20motionplanning,
    author = {Kim, Myeongseop and Han, Dong-Ki and Park, Jae-Han and Kim, Jung-Su},
    year = {2020},
    month = {01},
    pages = {575},
    title = {Motion Planning of Robot Manipulators for a Smoother Path Using a Twin Delayed Deep Deterministic Policy Gradient with Hindsight Experience Replay},
    volume = {10},
    journal = {Applied Sciences},
    doi = {10.3390/app10020575}
}
@inproceedings{Ng1999PolicyIU,
  title={Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  author={A. Ng and Daishi Harada and Stuart J. Russell},
  booktitle={International Conference on Machine Learning},
  year={1999},
  url={https://api.semanticscholar.org/CorpusID:5730166}
}
@misc{janner2021trust,
      title={When to Trust Your Model: Model-Based Policy Optimization}, 
      author={Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
      year={2021},
      eprint={1906.08253},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wang2019benchmarking,
      title={Benchmarking Model-Based Reinforcement Learning}, 
      author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
      year={2019},
      eprint={1907.02057},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{janner2019mbpo,
  author = {Michael Janner},
  title = {Model-Based Reinforcement Learning: Theory and Practice},
  year = {2019},
  url = {https://bair.berkeley.edu/blog/2019/12/12/mbpo/},
  note = {Accessed: 2024-06-05},
  howpublished = {Berkeley Artificial Intelligence Research (BAIR) Blog}
}
@misc{miller2023rewardshaping,
  author = {Miller, Tim},
  title = {Reward Shaping and Q-value Initialisation},
  year = {2023},
  howpublished = {The University of Queensland},
  note = {Available online; accessed 26 May 2024}
}
@misc{nilaksh2024barrier,
      title={Barrier Functions Inspired Reward Shaping for Reinforcement Learning}, 
      author={Nilaksh Nilaksh and Abhishek Ranjan and Shreenabh Agrawal and Aayush Jain and Pushpak Jagtap and Shishir Kolathaya},
      year={2024},
      eprint={2403.01410},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@online{rishi2024,
    author = "R. Rishi",
    title = "The Power of Model Compression: Guide to Pruning, Quantization, and Distillation in Machine Learning",
    howpublished = "Medium",
    month = 1,
    year = 2024,
    url = "https://medium.com/@thisisrishi/the-power-of-model-compression-guide-to-pruning-quantization-and-distillation-in-machine-dbc6d28bd3a3"
}
@mastersthesis{junior22,
  author       = {Junior Ndifor},
  title        = {Integrating Reinforcement Learning Algorithms in Constrained Embedded Devices to optimize Control for an Electric Floor Heating
System},
  school       = {University of Southern Denmark},
  year         = {2022},
  type         = {Master's Thesis}
}
@mastersthesis{seb23,
  author       = {Andres Sebastian Cespedes Cubides},
  title        = {Feasibility study of machine learning techniques applied to heating, ventilation and air conditioning systems},
  school       = {University of Southern Denmark},
  year         = {2023},
  type         = {Master's Thesis}
}
@misc{zhou2022single,
      title={Single Time-scale Actor-critic Method to Solve the Linear Quadratic Regulator with Convergence Guarantees}, 
      author={Mo Zhou and Jianfeng Lu},
      year={2022},
      eprint={2202.00048},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{MathWorksReward2021,
  author = {{MathWorks}},
  title = {Define Reward and Observation Signals in Custom Environments},
  year = {2021},
  howpublished = {\url{https://www.mathworks.com/help/reinforcement-learning/ug/define-reward-and-observation-signals.html}},
  note = {Accessed: 2024-05-20}
}
@article{watkins1992qlearning,
    author = {C. J. Watkins and P. Dayan},
    title = {Q-learning},
    journal = {Machine Learning},
    volume = {8},
    pages = {279--292},
    year = {1992}
}
@inproceedings{mnih2013playing,
    author = {V. Mnih and K. Kavukcuoglu and D. Silver and A. Graves and I. Antonoglou and D. Wierstra and M. Riedmiller},
    title = {Playing atari with deep reinforcement learning},
    booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
    year = {2013}
}
@inproceedings{faddel2020data,
    author = {S. Faddel and G. Tian and Q. Zhou and H. Aburub},
    title = {Data-driven q-learning for commercial HVAC control},
    booktitle = {2020 SoutheastCon},
    year = {2020},
    pages = {1--6}
}
@inproceedings{wei2017deep,
    author = {T. Wei and Y. Wang and Q. Zhu},
    title = {Deep reinforcement learning for building HVAC control},
    booktitle = {Proceedings of the 54th Annual Design Automation Conference},
    year = {2017},
    pages = {1--6}
}
@article{cho2006application,
    author = {S.-H. Cho},
    title = {Application study of reinforcement learning control for building HVAC system},
    journal = {International Journal of Air-Conditioning and Refrigeration},
    volume = {14},
    number = {4},
    pages = {138--146},
    year = {2006}
}
@inproceedings{li2015multi,
    author = {B. Li and L. Xia},
    title = {A multi-grid reinforcement learning method for energy conservation and comfort of HVAC in buildings},
    booktitle = {2015 IEEE International Conference on Automation Science and Engineering (CASE)},
    year = {2015},
    pages = {444--449}
}
@inproceedings{barrett2015autonomous,
    author = {E. Barrett and S. Linder},
    title = {Autonomous HVAC control, a reinforcement learning approach},
    booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015},
    year = {2015},
    series = {Proceedings, Part III},
    pages = {3--19}
}
@article{jia2019advanced,
  author = {R. Jia and M. Jin and K. Sun and T. Hong and C. Spanos},
  title = {Advanced building control via deep reinforcement learning},
  journal = {Energy Procedia},
  volume = {158},
  pages = {6158--6163},
  year = {2019}
}
@article{sierla2022review,
  author = {S. Sierla and H. Ihasalo and V. Vyatkin},
  title = {A Review of Reinforcement Learning Applications to Control of Heating, Ventilation and Air Conditioning Systems},
  journal = {Energies},
  volume = {15},
  number = {10},
  pages = {3526},
  year = {2022},
  doi = {10.3390/en15103526}
}
@article{paleyes2022challenges,
  author = {Andrei Paleyes and Raoul-Gabriel Urma and Neil D. Lawrence},
  title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
  journal = {Comput. Surveys},
  year = {2022}
}
@article{williams1992pg,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J.},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Kluwer Academic Publishers},
  doi={10.1007/BF00992696}
}
@misc{an2024beyondblackboxpolicies,
      title={Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control}, 
      author={Zhiyu An and Xianzhong Ding and Wan Du},
      year={2024},
      eprint={2403.00172},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}
@inproceedings{ding2019octopus,
  author = {Xianzhong Ding and Wan Du and Alberto Cerpa},
  title = {OCTOPUS: Deep Reinforcement Learning for Holistic Smart Building Control},
  booktitle = {ACM BuildSys},
  pages = {326--335},
  year = {2019}
}
@inproceedings{ding2020mb2c,
  author = {Xianzhong Ding and Wan Du and Alberto E. Cerpa},
  title = {Mb2c: Model-based Deep Reinforcement Learning for Multi-zone Building Control},
  booktitle = {ACM BuildSys},
  pages = {50--59},
  year = {2020}
}
@article{an2024reward,
  author = {Zhiyu An and others},
  title = {Reward Bound for Behavioral Guarantee of Model-based Planning Agents},
  journal = {arXiv preprint arXiv:2402.13419},
  year = {2024}
}
@misc{gu2016deep,
      title={Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates}, 
      author={Shixiang Gu and Ethan Holly and Timothy Lillicrap and Sergey Levine},
      year={2016},
      eprint={1610.00633},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@article{blad2019control,
  author = {C. Blad and S. Koch and S. Ganeswarathas and C. S. Kallesøe and S. Bøgh},
  title = {Control of HVAC-systems with Slow Thermodynamic Using Reinforcement Learning},
  journal = {Procedia Manufacturing},
  volume = {38},
  pages = {1308--1315},
  year = {2019},
  doi = {10.1016/j.promfg.2020.01.159}
}
@article{fu2018sarsa,
  author    = {F. Qiming and H. Lingyaoa and W. Hongjiea and H. Fuyuana and H. Wena and C. Jianpinga},
  title     = {A Sarsa-based Adaptive Controller for Building Energy Conservation},
  journal   = {Journal of Computational Methods in Sciences and Engineering},
  year      = {2018},
  volume    = {18},
  pages     = {329-338},
  month     = {5},
  day       = {3},
}
@misc{mnih2016asynchronous,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{haarnoja2019soft,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{huang2020hvac,
  author    = {Zetian Huang and Jianping Chen and Qiming Fu and Hongjie Wu and You Lu and Zhen Gao},
  title     = {HVAC Optimal Control with the Multistep-Actor Critic Algorithm in Large Action Spaces},
  journal   = {Mathematical Problems in Engineering},
  volume    = {2020},
  pages     = {Article ID 1386418, 12 pages},
  year      = {2020},
  doi       = {10.1155/2020/1386418},
}
@Article{en14040997,
    AUTHOR = {Coraci, Davide and Brandi, Silvio and Piscitelli, Marco Savino and Capozzoli, Alfonso},
    TITLE = {Online Implementation of a Soft Actor-Critic Agent to Enhance Indoor Temperature Control and Energy Efficiency in Buildings},
    JOURNAL = {Energies},
    VOLUME = {14},
    YEAR = {2021},
    NUMBER = {4},
    ARTICLE-NUMBER = {997},
    URL = {https://www.mdpi.com/1996-1073/14/4/997},
    ISSN = {1996-1073},
    DOI = {10.3390/en14040997}
}
@inproceedings{schulman2015trust,
  author    = {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  title     = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  year      = {2015},
  pages     = {1889-1897}
}
@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{wang2020truly,
  author = {Yuhui Wang and Hao He and Xiaoyang Tan},
  title = {Truly Proximal Policy Optimization},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  year = {2020},
  pages = {113--122},
  url = {http://proceedings.mlr.press/v115/wang20b/wang20b.pdf}
}
@misc{lillicrap2019ddpg,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{duyan20multiddpg,
    title = {Intelligent multi-zone residential HVAC control strategy based on deep reinforcement learning},
    author = {Du, Yan and Zandi, Helia and Kotevska, Olivera and Kurte, Kuldeep and Munk, Jeffery and Amasyali, Kadir and Mckee, Evan and Li, Fangxing},
    doi = {10.1016/j.apenergy.2020.116117},
    url = {https://www.osti.gov/biblio/1784145}, journal = {Applied Energy},
    issn = {0306-2619},
    number = 1,
    volume = 281,
    place = {United States},
    year = {2020},
    month = {11}
}
@article{fu21mpctd3,
    author = {Fu, Chenhui and Zhang, Yunhua},
    year = {2021},
    month = {09},
    pages = {1-1},
    title = {Research and Application of Predictive Control Method Based on Deep Reinforcement Learning for HVAC Systems},
    volume = {PP},
    journal = {IEEE Access},
    doi = {10.1109/ACCESS.2021.3114161}
}
@misc{manjavacas2024experimental,
      title={An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control}, 
      author={Antonio Manjavacas and Alejandro Campoy-Nieves and Javier Jiménez-Raboso and Miguel Molina-Solana and Juan Gómez-Romero},
      year={2024},
      eprint={2401.05737},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{eric03rwd,
    author = {Wiewiora, Eric},
    title = {Potential-based shaping and Q-value initialization are equivalent},
    year = {2003},
    issue_date = {July 2003},
    publisher = {AI Access Foundation},
    address = {El Segundo, CA, USA},
    volume = {19},
    number = {1},
    issn = {1076-9757},
    journal = {J. Artif. Int. Res.},
    month = {9},
    pages = {205–208},
    numpages = {4}
}